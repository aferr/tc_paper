\section{Evaluation}

We evaluated the timing compartments architecture using the gem5 architectural 
simulator~\cite{gem5} integrated with the DRAMSim2~\cite{DRAMSim2} 
memory simulator. Our experiments use multiprogram workloads 
comprised of SPEC2006 benchmarks compiled for the ARM ISA. 

Table~\ref{tab:config} shows our system configuration.
The cores use the gem5 ``O3`` out-of-order core model which runs at 2GHz. 
Each core has private 32KB L1 instruction and data caches, and private 256KB L2 
cache. The cores share a 4MB L3 cache. We derived cache configuration 
parameters from the Intel Xeon E3-1220L, which is a two core architecture used 
by Amazon EC2. In DRAMSim2, we simulate a 667MHz 2GB DDR3 memory. The 
interconnects in the simulator runs at 1GHz. Unless specified otherwise, each experiment is 
fastforwarded for 1 billion instructions, and run for 100 million 
instructions. We will first describe our security evaluation and then show
the performance evaluation.

\begin{table}
    \caption{Simulator configuration parameters.}
    \centering
    \begin{tabular}{|l|l|l|r|}
        \hline
        \multicolumn{3}{|l|}{gem5 core model} & ``O3''        \\\hline
        \multicolumn{3}{|l|}{CPU Clock}    & 2GHz             \\\hline
        \hline
        \multicolumn{2}{|l|}{Memory}             & 2GB    & 667MHz  \\\hline
        \hline
        \multicolumn{3}{|l|}{Network Clock}      & 1GHz \\\hline
        \hline
        L1d / L1i  & 32kB   & 2-way  & 2 cycles\\\hline
        L2         & 256kB  & 8-way  & 7 cycles \\\hline
        L3         & 4MB    & 16-way & 17 cycles  \\\hline
    \end{tabular}
    \label{tab:config}
\end{table}

\subsection{Security Evaluation}

To experimentally evaluate the security of the timing channel protection in our 
architecture, we use a two-core system with two timing compartments, TC0 and
TC1, running concurrently. The protection policy is configured to disallow any 
timing channels between the two compartments. If the number of cycles required 
to execute a certain number of instructions for a particular benchmark running on 
TC0 depends on which benchmark is running on TC1, it indicates timing 
interference exists that can be exploited to leak information about TC0. A 
secure system should guarantee the benchmark running in TC0 always uses the 
same number of cycles to finish regardless of which benchmark TC1 is running. 

Using the rule above, we evaluated the security of our architecture by running 
a fixed benchmark on TC0 while varying the benchmark on TC1. Then we compared 
the total execution time for the fixed benchmark in different runs. We 
evaluated our architecture as well as the baseline insecure architecture which 
does not implement any protection. As expected, the results for the baseline 
show the execution time of a particular benchmark on Core 0 changes significantly 
depending on the benchmark on Core 1, indicating timing channels exist between
the two cores. On the other hand, when running each pair of benchmarks with 
timing channel protection we observed no execution time difference by changing 
which program runs concurrently with TC0.

\subsection{Performance Evaluation}

The timing compartment architecture extends the insecure baseline with
static partitioning and time multiplexing to secure the shared hardware 
resources. These changes lead to underutilization, and thus, a performance
overhead. To evaluate the performance overhead, we ran pairs of
SPEC2006 benchmarks, and measured the execution time of the benchmark
in TC0. Since the timing channel protection mechanisms primarily affect the memory 
hierarchy, the memory intensity of the benchmarks is related to the performance 
overhead.
The memory intensity of each benchmark, in memory requests per thousand 
instructions, during the program segment used for our experiments is shown in
Figure~\ref{fig:memstudy}. Of the SPEC2006 benchmarks, mcf has the most memory 
requests, while astar has none after fast-forwarding for 1B instructions.

For the baseline architecture, we calculate the average execution time of the 
benchmark on Core 0 by averaging among runs with different benchmarks on Core 1. 
The execution time of the benchmark in Core 0 is impacted by the benchmark in 
Core 1. In the timing compartments architecture, the execution time of TC0 (on 
Core 0) is independent of workloads running in other timing compartments.

\begin{figure}
    \begin{center}
        \includegraphics[width=3.46in]{figs/memstudy.pdf}
        \caption{Memory intensity of SPEC2006 benchmarks.}
        \label{fig:memstudy}
    \end{center}
\end{figure}

Figure~\ref{fig:performance} shows the performance overhead of full system 
protection as well as the overhead incurred by the changes to the memory 
controller (mc), the cache (waypart), the L3 to memory bus (membus), and the L2 
to L3 bus (L2L3) individually compared to the insecure baseline. The
overheads for libquantum and mcf, which are particularly memory intensive, are 
highest at 69\% and 18\% respectively. However, the arithmetic mean of the 
total overhead is quite low at roughly 9\%. Memory controller protection incurs 
by far the most overhead suggesting the impact on memory bandwidth is more 
significant than bus or cache underutilization. Overall, the results show that 
the timing compartments are viable for applications that require high assurance 
for software isolation.

\begin{figure}
    \begin{center}
        \includegraphics[width=3.4in]{figs/breakdown.pdf}
        \caption{Performance overhead for 2 TCs.}
        \label{fig:performance}
    \end{center}
\end{figure}

The performance overhead is likely to increase as the number of timing 
compartment scales up. To study the impact of the number of timing 
compartments, we evaluated the performance overhead with three and four timing 
compartments on a 4-core  processor, each occupying one core and private L1 and 
L2 caches. They share a 4MB L3 cache. We do not evaluate all permutations of 4 
benchmarks. Instead, we only evaluate the subset of these permutations where 
TC1, TC2, and TC3 are executing the same benchmark. As before, each of these 
workloads with the same program in TC0 is averaged.

The performance overhead as the number of timing compartments increases is 
shown in Figure~\ref{fig:scalability}. The arithmetic mean of the overheads for 
each benchmark for 2, 3 and 4 timing compartments are 9\%, 17\%, and 24\% 
respectively. For libquantum which is particularly memory intensive, the 
overheads are 76\%, 140\%, and 184\% for 2, 3, and 4 timing compartments. The 
results suggest that the overhead of timing timing channel protection is rather 
insensitive to the number of timing compartments for compute-bound 
applications. For memory-intensive applications, the overhead can increase 
noticeably. However, the results still suggest that timing compartments can 
allow distrusting software entities to share hardware and provide better 
overall performance than running them on separate dedicated machines.

\begin{figure}
    \begin{center}
        \includegraphics[width=3.4in]{figs/scalability_split.pdf}
        \caption{Performance overhead for 4 TCs.}
        \label{fig:scalability}
    \end{center}
\end{figure}

While future multi-core systems are likely to have a large number of processing 
cores, we note that many security applications will not require many timing 
compartments to be used concurrently. For example, the BYOD application only 
requires two TCs no matter how many cores exist in the system. Similarly, in 
a high-assurance cloud computing environment, the cloud provider can limit the
number of high-assurance virtual machines that can be located on each physical
system while increasing the system utilization by co-locating non-secure
virtual machines with high-assurance ones.

